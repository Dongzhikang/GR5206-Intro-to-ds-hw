#     # … with 4 more rows
#     (Notice that here, the first rows of x and y actually have slightly different
#     values but when printed, they are rounded to `2.00` and `3.00`)
## Do not modify this line!
pts <- tribble(~method, ~optimum,
"gaussNewton", gaussNewton_minima,
"neldermead", neldermead_minima,
"optim", optim_minima)%>%
mutate(optimum = map(optimum,enframe))%>%
unnest(optimum)%>%
spread(name,value)%>%
rename('x'=`1`,'y'=`2`)%>%
mutate(fxy = map2(x,y,himmelblau))
pts
#     # A tibble: 7 x 4
#     method          x     y     fnxy
#     <chr>       <dbl> <dbl>    <dbl>
#     1 gaussNewton  3.00  2.00 2.10e- 6
#     2 neldermead   3.00  2.00 1.41e-11
#     3 optim        3.00  2.00 1.46e-12
#     # … with 4 more rows
#     (Notice that here, the first rows of x and y actually have slightly different
#     values but when printed, they are rounded to `2.00` and `3.00`)
## Do not modify this line!
pts <- tribble(~method, ~optimum,
"gaussNewton", gaussNewton_minima,
"neldermead", neldermead_minima,
"optim", optim_minima)%>%
mutate(optimum = map(optimum,enframe))%>%
unnest(optimum)%>%
spread(name,value)%>%
rename('x'=`1`,'y'=`2`)%>%
mutate(fxy = map(c(x,y),himmelblau))
#     # A tibble: 7 x 4
#     method          x     y     fnxy
#     <chr>       <dbl> <dbl>    <dbl>
#     1 gaussNewton  3.00  2.00 2.10e- 6
#     2 neldermead   3.00  2.00 1.41e-11
#     3 optim        3.00  2.00 1.46e-12
#     # … with 4 more rows
#     (Notice that here, the first rows of x and y actually have slightly different
#     values but when printed, they are rounded to `2.00` and `3.00`)
## Do not modify this line!
pts <- tribble(~method, ~optimum,
"gaussNewton", gaussNewton_minima,
"neldermead", neldermead_minima,
"optim", optim_minima)%>%
mutate(optimum = map(optimum,enframe))%>%
unnest(optimum)%>%
spread(name,value)%>%
rename('x'=`1`,'y'=`2`)%>%
mutate(fxy = map2(function(x,y) himmelblau(c(x,y))))
#     # A tibble: 7 x 4
#     method          x     y     fnxy
#     <chr>       <dbl> <dbl>    <dbl>
#     1 gaussNewton  3.00  2.00 2.10e- 6
#     2 neldermead   3.00  2.00 1.41e-11
#     3 optim        3.00  2.00 1.46e-12
#     # … with 4 more rows
#     (Notice that here, the first rows of x and y actually have slightly different
#     values but when printed, they are rounded to `2.00` and `3.00`)
## Do not modify this line!
pts <- tribble(~method, ~optimum,
"gaussNewton", gaussNewton_minima,
"neldermead", neldermead_minima,
"optim", optim_minima)%>%
mutate(optimum = map(optimum,enframe))%>%
unnest(optimum)%>%
spread(name,value)%>%
rename('x'=`1`,'y'=`2`)%>%
mutate(fxy = map2(x,y, function(x,y) himmelblau(c(x,y))))
pts
#     # A tibble: 7 x 4
#     method          x     y     fnxy
#     <chr>       <dbl> <dbl>    <dbl>
#     1 gaussNewton  3.00  2.00 2.10e- 6
#     2 neldermead   3.00  2.00 1.41e-11
#     3 optim        3.00  2.00 1.46e-12
#     # … with 4 more rows
#     (Notice that here, the first rows of x and y actually have slightly different
#     values but when printed, they are rounded to `2.00` and `3.00`)
## Do not modify this line!
pts <- tribble(~method, ~optimum,
"gaussNewton", gaussNewton_minima,
"neldermead", neldermead_minima,
"optim", optim_minima)%>%
mutate(optimum = map(optimum,enframe))%>%
unnest(optimum)%>%
spread(name,value)%>%
rename('x'=`1`,'y'=`2`)%>%
mutate(fxy = map2(x,y, function(x,y) himmelblau(c(x,y))))%>%
unnest(fxy)
pts
#     # A tibble: 7 x 4
#     method          x     y     fnxy
#     <chr>       <dbl> <dbl>    <dbl>
#     1 gaussNewton  3.00  2.00 2.10e- 6
#     2 neldermead   3.00  2.00 1.41e-11
#     3 optim        3.00  2.00 1.46e-12
#     # … with 4 more rows
#     (Notice that here, the first rows of x and y actually have slightly different
#     values but when printed, they are rounded to `2.00` and `3.00`)
## Do not modify this line!
pts <- tribble(~method, ~optimum,
"gaussNewton", gaussNewton_minima,
"neldermead", neldermead_minima,
"optim", optim_minima)%>%
bind_rows(c(softline_dir$method,softline_dir$optimum))
#     # A tibble: 7 x 4
#     method          x     y     fnxy
#     <chr>       <dbl> <dbl>    <dbl>
#     1 gaussNewton  3.00  2.00 2.10e- 6
#     2 neldermead   3.00  2.00 1.41e-11
#     3 optim        3.00  2.00 1.46e-12
#     # … with 4 more rows
#     (Notice that here, the first rows of x and y actually have slightly different
#     values but when printed, they are rounded to `2.00` and `3.00`)
## Do not modify this line!
pts <- tribble(~method, ~optimum,
"gaussNewton", gaussNewton_minima,
"neldermead", neldermead_minima,
"optim", optim_minima)%>%
bind_rows(select(softline_dir,method,optimum))
pts
#     # A tibble: 7 x 4
#     method          x     y     fnxy
#     <chr>       <dbl> <dbl>    <dbl>
#     1 gaussNewton  3.00  2.00 2.10e- 6
#     2 neldermead   3.00  2.00 1.41e-11
#     3 optim        3.00  2.00 1.46e-12
#     # … with 4 more rows
#     (Notice that here, the first rows of x and y actually have slightly different
#     values but when printed, they are rounded to `2.00` and `3.00`)
## Do not modify this line!
pts <- tribble(~method, ~optimum,
"gaussNewton", gaussNewton_minima,
"neldermead", neldermead_minima,
"optim", optim_minima)%>%
bind_rows(select(softline_dir,method,optimum))%>%
mutate(optimum = map(optimum,enframe))%>%
unnest(optimum)%>%
spread(name,value)%>%
rename('x'=`1`,'y'=`2`)%>%
mutate(fxy = map2(x,y, function(x,y) himmelblau(c(x,y))))%>%
unnest(fxy)
pts
#       and `subtitle` to `"Softline allow to explore different directions"`.
#     - `theme_light()` to set light background.
#     Save the plot to `compare_plot`.
#     We can see that `gaussNewton`, `neldemead` and `optim` can converge to a good
#     local minima in the upper right corner while the `softline` converge worse
#     compared to them. However, `softline` can be useful if we want to implement
#     gradient descent in helping us find suitable step size in each step, so that
#     we can search for the best direction while optimizing how much we want to step
#     in that direction.
## Do not modify this line!
compare_plot <-contour+
geom_point(data = pts,aes(color=method),size=2)+
labs(title = "Methods find different local minima",
subtitle = "Softline allow to explore different directions")+
theme_light()
compare_plot
#     # A tibble: 7 x 4
#     method          x     y     fnxy
#     <chr>       <dbl> <dbl>    <dbl>
#     1 gaussNewton  3.00  2.00 2.10e- 6
#     2 neldermead   3.00  2.00 1.41e-11
#     3 optim        3.00  2.00 1.46e-12
#     # … with 4 more rows
#     (Notice that here, the first rows of x and y actually have slightly different
#     values but when printed, they are rounded to `2.00` and `3.00`)
## Do not modify this line!
pts <- tribble(~method, ~optimum,
"gaussNewton", gaussNewton_minima,
"neldermead", neldermead_minima,
"optim", optim_minima)%>%
bind_rows(select(softline_dir,method,optimum))%>%
mutate(optimum = map(optimum,enframe))%>%
unnest(optimum)%>%
spread(name,value)%>%
rename('x'=`1`,'y'=`2`)%>%
mutate(fnxy = map2(x,y, function(x,y) himmelblau(c(x,y))))%>%
unnest(fnxy)
pts
himmelblau(gaussNewton_minima)
#       and `subtitle` to `"Softline allow to explore different directions"`.
#     - `theme_light()` to set light background.
#     Save the plot to `compare_plot`.
#     We can see that `gaussNewton`, `neldemead` and `optim` can converge to a good
#     local minima in the upper right corner while the `softline` converge worse
#     compared to them. However, `softline` can be useful if we want to implement
#     gradient descent in helping us find suitable step size in each step, so that
#     we can search for the best direction while optimizing how much we want to step
#     in that direction.
## Do not modify this line!
compare_plot <-contour+
geom_point(data = pts,aes(color=method),size=2)+
labs(title = "Methods find different local minima",
subtitle = "Softline allow to explore different directions")+
theme_light()
compare_plot
#    (hint: use the command `seed <- .Random.seed`).
#    Generate 1000 samples from a log-normal distribution
#    with parameters `meanlog = 2`, `sdlog = 1`, add with random
#    noise from a normal distribution with mean 0 and standard deviation 0.05.
#    To do this, you should use:
#      - `set.seed()` to set random seed
#      - `rlnorm()` to generate random variables from log-normal distribution
#      - `rnorm()` to generate random variables from normal distribution
#    Save the generated vector into `x`.
## Do not modify this line!
set.seed(0)
seed <- .Random.seed
x <- rlnorm(1000,meanlog = 2,sdlog=1)+rnorm(1000,0,.05)
#    Note: `expectation = exp(meanlog + 0.5 * sdlog^2)`,
#          `variance = exp(2 * meanlog + sdlog^2) * (exp(sdlog^2) - 1)`.
#    Example: `mom_lnorm(c(1, 1))` would return `[1]  4.481689 34.512613`.
#    Then create a function factory `obj_lnorm_factory` that takes an input `x`
#    representing the data, and return a function which takes an input `par` as
#    parameter and returns the sum of squared error between the
#    true mean and variance of `x` and the log-normal distribution
#    specified by `par`.
#    Example: `obj_lnorm_factory(c(0, 1))(c(1, 1))` would return `[1] 1172.712`.
## Do not modify this line!
mom_lnorm <- function(par) c(exp(par[1]+.5*par[2]**2),exp(2*par[1]+par[2]^2)*(exp(par[2]^2)-1))
mom_lnorm(c(1, 1))
#    Note: `expectation = exp(meanlog + 0.5 * sdlog^2)`,
#          `variance = exp(2 * meanlog + sdlog^2) * (exp(sdlog^2) - 1)`.
#    Example: `mom_lnorm(c(1, 1))` would return `[1]  4.481689 34.512613`.
#    Then create a function factory `obj_lnorm_factory` that takes an input `x`
#    representing the data, and return a function which takes an input `par` as
#    parameter and returns the sum of squared error between the
#    true mean and variance of `x` and the log-normal distribution
#    specified by `par`.
#    Example: `obj_lnorm_factory(c(0, 1))(c(1, 1))` would return `[1] 1172.712`.
## Do not modify this line!
mom_lnorm <- function(par) c(exp(par[1]+.5*par[2]**2),exp(2*par[1]+par[2]^2)*(exp(par[2]^2)-1))
obj_lnorm_factory <- function(x){
function(par) (mom_lnorm(par)[1]-mean(x))^2+(mom_lnorm(par)[2]-var(x))^2
}
obj_lnorm_factory(c(0, 1))(c(1, 1))
#      - Set `fn` to `obj_lnorm_factory(x)`.
#      - Set `hessian` to `TRUE`.
#    - `solve()` to compute the inverse of hessian matrix.
#    - `sqrt()` and `diag()` to compute the standard error.
#    - `tibble()` to create a tibble.
#    (2) Use `par_lnorm` with the data `x` simulated in question 1 to estimate
#    the moments. Save the tibble to `result_mom`.
#    Save the estimate of `meanlog` to scalar `mu_mom`.
#    Save the estimate of `sdlog` to scalar `sd_mom`.
## Do not modify this line!
par_lnorm <- function(x) {
op <- optim(c(1,1),obj_lnorm_factory(x),hessian = T)
value <- op$par
se <- sqrt(diag(solve(op$hessian)))
lower <- value-1.96*se
upper <- value+1.96*se
return(tibble(parameter=c("mu_mom","se_mom"),
value = value,
se = se,
lower = lower,
upper = upper))
}
par_lnorm(x)
#      - Set `fn` to `obj_lnorm_factory(x)`.
#      - Set `hessian` to `TRUE`.
#    - `solve()` to compute the inverse of hessian matrix.
#    - `sqrt()` and `diag()` to compute the standard error.
#    - `tibble()` to create a tibble.
#    (2) Use `par_lnorm` with the data `x` simulated in question 1 to estimate
#    the moments. Save the tibble to `result_mom`.
#    Save the estimate of `meanlog` to scalar `mu_mom`.
#    Save the estimate of `sdlog` to scalar `sd_mom`.
## Do not modify this line!
par_lnorm <- function(x) {
op <- optim(c(1,1),obj_lnorm_factory(x),hessian = T)
value <- op$par
se <- sqrt(diag(solve(op$hessian)))
lower <- value-1.96*se
upper <- value+1.96*se
return(tibble(parameter=c("mu_mom","sd_mom"),
value = value,
se = se,
lower = lower,
upper = upper))
}
result_mom <- par_lnorm(x)
result_mom
result_mom[mu_mom]
result_mom['mu_mom']
result_mom[1,]
mu_mom <- result_mom[[1,]]
mu_mom
mu_mom <- result_mom[[1:3]]
mu_mom <- value(result_mom)
mu_mom <-
sd_mom <- c(result_mom[2,])
mu_mom <-
sd_mom <- c(result_mom[2,])
mu_mom <- c(result_mom[1,])
mu_mom
mu_mom <- as.vector(result_mom[1,])
mu_mom
mu_mom <- as.numeric(result_mom[1,])
mu_mom
mu_mom <- as.numeric(result_mom[1,-1])
mu_mom
mu_mom <- as.numeric(result_mom[1,2])
sd_mom <- as.numeric(result_mom[2,2])
#    of our data under the parameters. The function will take two inputs `par`,
#    a vector of length two representing the moments, and `x`, the data.
#    It returns the negative loglikelihood.
#    To do this, you can use:
#      - `dlnorm()` to calculate the likelihood
#      - specify `log = TRUE` to calculate the loglikelihood.
#    Then use your `nll_lnorm()` and `optim()` to estimate the parameters
#    (i.e. minimize the negative loglikelihood). Choose your startpoint to be `c(1, 1)`.
#    Save your estimated parameter `meanlog` into `mu_mle` and `sdlog` into `sd_mle`.
## Do not modify this line!
nll_lnorm <- function(par,x) -sum(dlnorm(x,par[1],par[2],log=T))
op <-optim(c(1, 1),nll_lnorm(x))
op <-optim(c(1, 1),nll_lnorm,x=x)
mu_mle <- op$par[1]
sd_mle <- op$par[2]
#      - `labs()` to format the labels such that:
#        - `title = "Both methods seem to do a good job"`.
#        - `x = "Simulated x"`.
#        - `y = "Density"`.
#      - `theme_light()` to change the theme of plots.
#      - `theme()` to change the title and subtitle to the middle of the plot.
#        - Set its argument `plot.title` using `element_text(hjust = 0.5)`.
#    Save your figure into `compare_plot`.
#
## Do not modify this line!
compare_plot <- ggplot(as.data.frame(x),aes(x=x))+
geom_histogram(aes(y=..density..),bins=50)+
geom_density(color="Data")+
stat_function(color="MOM",fun = obj_lnorm_factory,args = list(x=x))
compare_plot
#      - `labs()` to format the labels such that:
#        - `title = "Both methods seem to do a good job"`.
#        - `x = "Simulated x"`.
#        - `y = "Density"`.
#      - `theme_light()` to change the theme of plots.
#      - `theme()` to change the title and subtitle to the middle of the plot.
#        - Set its argument `plot.title` using `element_text(hjust = 0.5)`.
#    Save your figure into `compare_plot`.
#
## Do not modify this line!
compare_plot <- ggplot(as.data.frame(x),aes(x=x))+
geom_histogram(aes(y=..density..),bins=50)+
geom_density(aes(x),color="Data")+
stat_function(color="MOM",fun = obj_lnorm_factory,args = list(x=x))
compare_plot
#      - `labs()` to format the labels such that:
#        - `title = "Both methods seem to do a good job"`.
#        - `x = "Simulated x"`.
#        - `y = "Density"`.
#      - `theme_light()` to change the theme of plots.
#      - `theme()` to change the title and subtitle to the middle of the plot.
#        - Set its argument `plot.title` using `element_text(hjust = 0.5)`.
#    Save your figure into `compare_plot`.
#
## Do not modify this line!
compare_plot <- ggplot(as.data.frame(x),aes(x=x))+
geom_histogram(aes(y=..density..),bins=50)+
geom_density(aes(x,color="Data"))+
stat_function(color="MOM",fun = obj_lnorm_factory,args = list(x=x))
compare_plot
#      - `labs()` to format the labels such that:
#        - `title = "Both methods seem to do a good job"`.
#        - `x = "Simulated x"`.
#        - `y = "Density"`.
#      - `theme_light()` to change the theme of plots.
#      - `theme()` to change the title and subtitle to the middle of the plot.
#        - Set its argument `plot.title` using `element_text(hjust = 0.5)`.
#    Save your figure into `compare_plot`.
#
## Do not modify this line!
compare_plot <- ggplot(as.data.frame(x),aes(x=x))+
geom_histogram(aes(y=..density..),bins=50)+
geom_density(aes(color="Data"))+
stat_function(aes(color="MOM"),fun = obj_lnorm_factory,args = list(x=x))
compare_plot
#      - `labs()` to format the labels such that:
#        - `title = "Both methods seem to do a good job"`.
#        - `x = "Simulated x"`.
#        - `y = "Density"`.
#      - `theme_light()` to change the theme of plots.
#      - `theme()` to change the title and subtitle to the middle of the plot.
#        - Set its argument `plot.title` using `element_text(hjust = 0.5)`.
#    Save your figure into `compare_plot`.
#
## Do not modify this line!
compare_plot <- ggplot(as.data.frame(x),aes(x=x))+
geom_histogram(aes(y=..density..),bins=50)+
geom_density(aes(color="Data"))+
stat_function(aes(color="MOM"),fun = dlnorm,args = list(meanlog=mu_mom,sdlog=sd_mom))+
stat_function(aes(color="MLE"),fun = dlnorm,args = list(meanlog=mu_mle,sdlog=sd_mle))+
labs(title = "Both methods seem to do a good job",
x = "Simulated x",
y = "Density")+
theme_light()+
theme(plot.title = element_text(hjust = 0.5))
compare_plot
View(op)
sqrt(17)*0.2/sqrt(0.095625)
8/3
1-pt(2.667,16)
sqrt(17)*0.1/sqrt(0.095625)
pt(-1.3333,16)
1-pt(1.3333,16)
pt(-1.3333,16)*2
source('~/Documents/CU 2019 Fall/Intro to DS/GR5206-Intro-to-ds-hw/HW9/hw9_poisson_distribution.R')
#    - create a vector by respectively substracting and adding the value
#      computed in the previous step to the `mle_trunc_poisson` and assign it
#      to `ci_mle_trunc_poisson`
#    $$fisher_info = \frac{(1-exp(-mle))^2 - mle^{2}*exp(-mle)}{mle*(1-exp(-mle) - mle*exp(-mle))^2}$$
#    where `mle` is the Maximum Likelihood Estimator (ie. `mle_trunc_poisson`
#    in this exercise)
#    Note: the fisher info is equal to the second derivative of the negative
#    log-likelihood evaluated at lambda equal to `mle_trunc_poisson`,
## Do not modify this line!
#mle_trunc_poisson <-optimize(nll_trunc_poisson,c(1,1.8))$minimum
mle_trunc_poisson <- 1.4+1.63*10^(-6)-0.00161+1.3*10^(-6)
mle <- mle_trunc_poisson
#fisher_info <- ((1-exp(-mle_trunc_poisson))^2-mle_trunc_poisson^2*exp(-mle_trunc_poisson))/((mle_trunc_poisson*(1-exp(-mle_trunc_poisson))-mle_trunc_poisson*exp(-mle_trunc_poisson)))^2
fisher_info <-((1-exp(-mle))^2 - mle^{2}*exp(-mle))/(mle*(1-exp(-mle) - mle*exp(-mle))^2)
fisher_info <- ((1-exp(-mle_trunc_poisson))^2-mle_trunc_poisson^2*exp(-mle_trunc_poisson))/(((mle_trunc_poisson*(1-exp(-mle_trunc_poisson))-mle_trunc_poisson*exp(-mle_trunc_poisson)))^2)
#    - create a vector by respectively substracting and adding the value
#      computed in the previous step to the `mle_trunc_poisson` and assign it
#      to `ci_mle_trunc_poisson`
#    $$fisher_info = \frac{(1-exp(-mle))^2 - mle^{2}*exp(-mle)}{mle*(1-exp(-mle) - mle*exp(-mle))^2}$$
#    where `mle` is the Maximum Likelihood Estimator (ie. `mle_trunc_poisson`
#    in this exercise)
#    Note: the fisher info is equal to the second derivative of the negative
#    log-likelihood evaluated at lambda equal to `mle_trunc_poisson`,
## Do not modify this line!
#mle_trunc_poisson <-optimize(nll_trunc_poisson,c(1,1.8))$minimum
mle_trunc_poisson <- 1.4+1.63*10^(-6)-0.00161+1.3*10^(-6)
ml <- mle_trunc_poisson
fisher_info <-((1-exp(-ml))^2 - ml^{2}*exp(-ml))/(ml*(1-exp(-ml) - ml*exp(-ml))^2)
n <- length(discoveries)
ci_mle_trunc_poisson <-c(mle_trunc_poisson+n*qnorm(.975),mle_trunc_poisson-n*qnorm(.975))
chisq.test(discoveries_count,trunc_poisson,simulate.p.value=T)$p.value
source('~/Documents/CU 2019 Fall/Intro to DS/GR5206-Intro-to-ds-hw/HW9/hw9_poisson_distribution.R')
chisq.test(discoveries_count,trunc_poisson,simulate.p.value=T)$p.value
chisq_p_value <-chisq.test(discoveries_count,trunc_poisson,simulate.p.value=T)$p.value
chisq.test(discoveries_count,trunc_poisson,simulate.p.value=T)$p.value
chisq.test(trunc_poisson,discoveries_count,simulate.p.value=T)$p.value
#    Compute the rest of the probability mass (ie. `P(X > 9)`) and assign the
#    probability to `prob_mass_beyond_9`.
#    Hint: you can use  `trunc_poisson_to_9` and the fact that the
#    probabilities sum to 1.
#    Add `prob_mass_beyond_9` to the `trunc_poisson_to_9` vector and name the
#    resulting vector `trunc_poisson`.
#    `trunc_poisson` should print to:
#    [1] 5.924706e-01 2.761689e-01 9.654815e-02 2.700245e-02 6.293339e-03
#    [6] 1.257223e-03 2.197615e-04 3.414588e-05 5.461141e-06
## Do not modify this line!
trunc_poisson_to_9 <- dtrunc_poisson(2:9, mle_trunc_poisson)
prob_mass_beyond_9 <- 1-sum(dtrunc_poisson(2:9,mle_trunc_poisson))
trunc_poisson <- c(trunc_poisson_to_9, prob_mass_beyond_9)
trunc_poisson
#    - create a vector by respectively substracting and adding the value
#      computed in the previous step to the `mle_trunc_poisson` and assign it
#      to `ci_mle_trunc_poisson`
#    $$fisher_info = \frac{(1-exp(-mle))^2 - mle^{2}*exp(-mle)}{mle*(1-exp(-mle) - mle*exp(-mle))^2}$$
#    where `mle` is the Maximum Likelihood Estimator (ie. `mle_trunc_poisson`
#    in this exercise)
#    Note: the fisher info is equal to the second derivative of the negative
#    log-likelihood evaluated at lambda equal to `mle_trunc_poisson`,
## Do not modify this line!
#mle_trunc_poisson <-optimize(nll_trunc_poisson,c(1,1.8))$minimum
mle_trunc_poisson <- 1.4+1.63*10^(-6)-0.00161+1.3*10^(-6)
ml <- mle_trunc_poisson
fisher_info <-((1-exp(-ml))^2 - ml^{2}*exp(-ml))/(ml*(1-exp(-ml) - ml*exp(-ml))^2)
n <- length(discoveries)
ci_mle_trunc_poisson <-c(mle_trunc_poisson+n*qnorm(.975),mle_trunc_poisson-n*qnorm(.975))
chisq.test(discoveries_count,trunc_poisson,simulate.p.value=T)
1-chisq.test(discoveries_count,trunc_poisson,simulate.p.value=T)
#    stochasticity and why we need to set the seed before the call).
#    Retrieve its p-value and assign it to `chisq_p_value`.
#    You should get a p-value significantly smaller than 0.05. This tells us
#    that we can reject the hypothesis that the distribution fits the data
#    well with high confidence. It seems that the paper's choice of
#    distribution to model the data at hand is maybe not the best to explain
#    its trends. Further goodness-of-fit tests could confirm this intuition.
#
## Do not modify this line!
#set.seed(0)
chisq_p_value <-chisq.test(discoveries_count,trunc_poisson,simulate.p.value=T)$p.value
1-chisq.test(discoveries_count,trunc_poisson,simulate.p.value=T)
chisq.test(discoveries_count,trunc_poisson,simulate.p.value=T)
chisq.test(discoveries_count,trunc_poisson,simulate.p.value=T)
chisq.test(discoveries_count,trunc_poisson,simulate.p.value=T)
discoveries_count
trunc_poisson
#    will be computed by Monte-Carlo simulation - this is where there is
#    stochasticity and why we need to set the seed before the call).
#    Retrieve its p-value and assign it to `chisq_p_value`.
#    You should get a p-value significantly smaller than 0.05. This tells us
#    that we can reject the hypothesis that the distribution fits the data
#    well with high confidence. It seems that the paper's choice of
#    distribution to model the data at hand is maybe not the best to explain
#    its trends. Further goodness-of-fit tests could confirm this intuition.
#
## Do not modify this line!
set.seed(0)
chisq.test(discoveries_count,trunc_poisson)
chisq.test(discoveries_count,trunc_poisson,p.value=T)
chisq.test(discoveries_count,trunc_poisson,simulate.p.value=T)
